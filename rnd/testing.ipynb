{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import from_json, col \n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "# 1. create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreamingETL\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Matikan pemanggilan native Windows API untuk Hadoop (biar gak error nativeio)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"hadoop.native.lib\", \"false\")\n",
    "\n",
    "# 3. read from kafka \n",
    "kafka_stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9093\") \\\n",
    "    .option(\"subscribe\", \"server.public.region, server.public.territory\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# 4. Ambil kolom value & convert ke string\n",
    "kafka_df = kafka_stream_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Schema yang sesuai struktur Kafka JSON (dengan schema & payload)\n",
    "territory_kafka_schema = StructType([\n",
    "    StructField(\"schema\", StructType([])),  # Optional, kita abaikan\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"territoryid\", IntegerType(), False),\n",
    "        StructField(\"territorydescription\", StringType(), True),\n",
    "        StructField(\"regionid\", IntegerType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "\n",
    "# Define schema sesuai struktur kafka JSON \n",
    "region_kafka_schema = StructType([\n",
    "    StructField(\"schema\", StructType([])),  # Optional, kita abaikan\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"regionid\", IntegerType(), False),\n",
    "        StructField(\"regiondescription\", StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Parse JSON dari Kafka messages menjadi DataFrame\n",
    "territory_parsed_df = kafka_df \\\n",
    "    .filter(col('value').contains('\"territoryid\"')) \\\n",
    "    .select(from_json(col('value'), territory_kafka_schema).alias('json')) \\\n",
    "    .select(\"json.payload.*\")\n",
    "# region_parsed_df = kafka_df2 \\\n",
    "#     .filter(col('value').contains('\"regionid\"')) \\\n",
    "#     .select(from_json(col('value'), region_kafka_schema).alias('json')) \\\n",
    "#     .select(\"json.payload.*\")\n",
    "\n",
    "\n",
    "query = territory_parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
